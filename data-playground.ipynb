{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc416e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import json\n",
    "import psutil\n",
    "import datasets\n",
    "from datasets import Dataset as Dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e14bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "class myTokenizer():\n",
    "    \"\"\"\n",
    "    Load tokenizer from bert config or defined BPE vocab dict\n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    ### You can custome your own tokenizer here. ###\n",
    "    ################################################\n",
    "    def __init__(self, vocab, config_name):\n",
    "        if vocab == 'bert':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config_name)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "    \n",
    "    def encode_token(self, sentences):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            input_ids = [[0] + [self.tokenizer.get(x, self.tokenizer['[UNK]']) for x in seq.split()] + [1] for seq in sentences]\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            input_ids = self.tokenizer(sentences, add_special_tokens=True)['input_ids']\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return input_ids\n",
    "        \n",
    "    def decode_token(self, seq):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = \" \".join([self.rev_tokenizer[x] for x in seq]).replace('__ ', '').replace('@@ ', '')\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = self.tokenizer.decode(seq)\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def load_model_emb(hidden_dim, tokenizer):\n",
    "    ### random emb or pre-defined embedding like glove embedding. You can custome your own init here.\n",
    "    model = torch.nn.Embedding(tokenizer.vocab_size, hidden_dim)\n",
    "    torch.nn.init.normal_(model.weight)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_tokenizer(vocab, config_name):\n",
    "    tokenizer = myTokenizer(vocab, config_name)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f8c0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab='bert'\n",
    "config_name='bert-base-uncased'\n",
    "hidden_dim=64\n",
    "data_dir='datasets/CommonsenseConversation'\n",
    "batch_size=10\n",
    "seq_len=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(vocab, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ad7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca000e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5c385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fea8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_text(\n",
    "    batch_size, \n",
    "    seq_len, \n",
    "    data_dir,\n",
    "    deterministic=False, \n",
    "    data_args=None, \n",
    "    model_emb=None,\n",
    "    split='train', \n",
    "    loaded_vocab=None,\n",
    "    loop=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a dataset, create a generator over (seqs, kwargs) pairs.\n",
    "\n",
    "    Each seq is an (bsz, len, h) float tensor, and the kwargs dict contains zero or\n",
    "    more keys, each of which map to a batched Tensor of their own.\n",
    "    The kwargs dict can be used for some meta information.\n",
    "\n",
    "    :param batch_size: the batch size of each returned pair.\n",
    "    :param seq_len: the max sequence length (one-side).\n",
    "    :param deterministic: if True, yield results in a deterministic order.\n",
    "    :param data_args: including dataset directory, num of dataset, basic settings, etc.\n",
    "    :param model_emb: loaded word embeddings.\n",
    "    :param loaded_vocab: loaded word vocabs.\n",
    "    :param loop: loop to get batch data or not.\n",
    "    \"\"\"\n",
    "\n",
    "    print('#'*30, '\\nLoading text data...')\n",
    "\n",
    "    training_data = get_corpus(data_dir, seq_len, split=split, loaded_vocab=loaded_vocab)\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        training_data,\n",
    "        model_emb=model_emb\n",
    "    )\n",
    "\n",
    "    if split != 'test':\n",
    "#         sampler = DistributedSampler(dataset)\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "#             sampler=sampler,\n",
    "            # shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "    else:\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "            # sampler=sampler,\n",
    "            shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    if loop:\n",
    "        return infinite_loader(data_loader)\n",
    "    else:\n",
    "        # print(data_loader)\n",
    "        return iter(data_loader)\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    while True:\n",
    "        yield from data_loader\n",
    "\n",
    "def helper_tokenize(sentence_lst, vocab_dict, seq_len):\n",
    "    # Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    raw_datasets = Dataset2.from_dict(sentence_lst)\n",
    "    print(raw_datasets)\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        input_id_x = vocab_dict.encode_token(examples['src'])\n",
    "        input_id_y = vocab_dict.encode_token(examples['trg'])\n",
    "        result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=['src', 'trg'],\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print('### tokenized_datasets', tokenized_datasets)\n",
    "    print('### tokenized_datasets...example', tokenized_datasets['input_id_x'][0])\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "\n",
    "            lst.append(src + [vocab_dict.sep_token_id] + trg)\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
    "    def pad_function(group_lst):\n",
    "        max_length = seq_len\n",
    "        group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], vocab_dict.pad_token_id, max_length)\n",
    "        group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "        return group_lst\n",
    "\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n",
    "\n",
    "    print(lm_datasets, 'padded dataset')\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict()\n",
    "    raw_datasets['train'] = lm_datasets\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def get_corpus(data_dir, seq_len, split='train', loaded_vocab=None):\n",
    "\n",
    "    print('#'*30, '\\nLoading dataset from {}...'.format(data_dir))\n",
    "\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    \n",
    "    if split == 'train':\n",
    "        print('### Loading form the TRAIN set...')\n",
    "        path = f'{data_dir}/train.jsonl'\n",
    "    elif split == 'valid':\n",
    "        print('### Loading form the VALID set...')\n",
    "        path = f'{data_dir}/valid.jsonl'\n",
    "    elif split == 'test':\n",
    "        print('### Loading form the TEST set...')\n",
    "        path = f'{data_dir}/test.jsonl'\n",
    "    else:\n",
    "        assert False, \"invalid split for dataset\"\n",
    "\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "    print('### Data samples...\\n', sentence_lst['src'][:2], sentence_lst['trg'][:2])\n",
    "        \n",
    "    # get tokenizer.\n",
    "    vocab_dict = loaded_vocab\n",
    "\n",
    "    train_dataset = helper_tokenize(sentence_lst, vocab_dict, seq_len)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af70dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from datasets/CommonsenseConversation...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " [\"this is my favorite story arc . ca n't wait to see how he does in the tourney ! the show is my guarantee smile for the week .\", 'it gives developers of a vr - centric title a baseline performance to aim for at the very least .'] [\"yea it 's hard not to have a smile on your face the entire episode\", \"is n't that exactly what pc gamers hate about consoles ? having one piece of hardware keeping technology behind instead of aiming for the best .\"]\n",
      "RAM used: 281.88 MB\n",
      "Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 19\n",
      "})\n",
      "RAM used: 282.46 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3305a60ffa455bba2971cd5ff65675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 19\n",
      "})\n",
      "### tokenized_datasets...example [101, 2023, 2003, 2026, 5440, 2466, 8115, 1012, 6187, 1050, 1005, 1056, 3524, 2000, 2156, 2129, 2002, 2515, 1999, 1996, 2778, 5420, 999, 1996, 2265, 2003, 2026, 11302, 2868, 2005, 1996, 2733, 1012, 102]\n",
      "RAM used: 282.62 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5567c94f60bc463da1039d6619c00e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 283.62 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a57a700d8c431bbd42a4a0a729ac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 19\n",
      "}) padded dataset\n",
      "RAM used: 283.80 MB\n",
      "RAM used: 283.88 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8c0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
